{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model for dense embeddings (using a pre-trained model like BERT)\n",
    "model_name = \"bert-base-uncased\"  # You can choose another pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the last hidden state as the dense representation\n",
    "    dense_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return dense_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_representation_tfidf(corpus, text, vocab_size=512):\n",
    "    vectorizer = TfidfVectorizer(max_features=vocab_size)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    text_vector = vectorizer.transform([text])\n",
    "    sparse_representation = text_vector.toarray().flatten()\n",
    "    \n",
    "    # Ensure consistent dimension\n",
    "    if sparse_representation.shape[0] < vocab_size:\n",
    "        sparse_representation = np.pad(sparse_representation, (0, vocab_size - sparse_representation.shape[0]), 'constant')\n",
    "    elif sparse_representation.shape[0] > vocab_size:\n",
    "        sparse_representation = sparse_representation[:vocab_size]\n",
    "\n",
    "    return sparse_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_tfidf(question, corpus):\n",
    "    # Get dense embedding for the question\n",
    "    question_dense = get_dense_embedding(question)\n",
    "    \n",
    "    # Get sparse representation for the question using TF-IDF\n",
    "    question_sparse = get_sparse_representation_tfidf(corpus, question)\n",
    "\n",
    "    scores = []\n",
    "    for doc in corpus:\n",
    "        # Dense similarity\n",
    "        doc_dense = get_dense_embedding(doc)\n",
    "        dense_score = cosine_similarity(question_dense.reshape(1, -1), doc_dense.reshape(1, -1))[0][0]\n",
    "        \n",
    "        # Sparse similarity\n",
    "        doc_sparse = get_sparse_representation_tfidf(corpus, doc)\n",
    "        sparse_score = cosine_similarity(question_sparse.reshape(1, -1), doc_sparse.reshape(1, -1))[0][0]\n",
    "\n",
    "        # Hybrid score: weighted sum of dense and sparse scores\n",
    "        combined_score = 0.6 * dense_score + 0.4 * sparse_score\n",
    "        scores.append((doc, combined_score))\n",
    "\n",
    "    # Sort documents by similarity score in descending order\n",
    "    sorted_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A young girl dreams of exploring distant planets one day, imagining vibrant landscapes and alien creatures far beyond the stars.', 0.34985727816148393), ('In a quiet library, a scholar researched ancient texts, delving into forgotten knowledge that shaped the foundations of modern thought.', 0.23992466926574707), ('The ancient tree stood tall, witnessing countless seasons change, its leaves whispering secrets of the past to the wind.', 0.2341342806816101), ('The sun set behind the mountains, painting the sky orange and purple, as the world prepared for the peaceful night ahead.', 0.2119516611099243), ('The teacher explained complex math problems to eager students, using engaging examples and visual aids to make learning enjoyable and effective.', 0.19984111189842224), ('A curious cat watched the birds from the window sill, its eyes wide with fascination as they chirped and flew around.', 0.18599746227264405), ('The dog barked excitedly as the mailman approached, wagging its tail vigorously, ready to greet him with playful enthusiasm.', 0.17294504642486572), ('The artist painted a beautiful landscape on canvas, capturing the vibrant colors of the sunset reflecting on the serene lake.', 0.16408220529556275), ('In the kitchen, the chef prepared a gourmet meal with fresh ingredients, creating an explosion of flavors that delighted the senses.', 0.1627883434295654), ('A musician strummed his guitar, creating a soothing melody that echoed through the room, captivating everyone with its heartfelt notes.', 0.1584308624267578)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "corpus = [\n",
    "    \"The ancient tree stood tall, witnessing countless seasons change, its leaves whispering secrets of the past to the wind.\",\n",
    "    \"In the kitchen, the chef prepared a gourmet meal with fresh ingredients, creating an explosion of flavors that delighted the senses.\",\n",
    "    \"A young girl dreams of exploring distant planets one day, imagining vibrant landscapes and alien creatures far beyond the stars.\",\n",
    "    \"The artist painted a beautiful landscape on canvas, capturing the vibrant colors of the sunset reflecting on the serene lake.\",\n",
    "    \"A curious cat watched the birds from the window sill, its eyes wide with fascination as they chirped and flew around.\",\n",
    "    \"The teacher explained complex math problems to eager students, using engaging examples and visual aids to make learning enjoyable and effective.\",\n",
    "    \"In a quiet library, a scholar researched ancient texts, delving into forgotten knowledge that shaped the foundations of modern thought.\",\n",
    "    \"The sun set behind the mountains, painting the sky orange and purple, as the world prepared for the peaceful night ahead.\",\n",
    "    \"A musician strummed his guitar, creating a soothing melody that echoed through the room, captivating everyone with its heartfelt notes.\",\n",
    "    \"The dog barked excitedly as the mailman approached, wagging its tail vigorously, ready to greet him with playful enthusiasm.\"\n",
    "]\n",
    "question = \"Who dreams\"\n",
    "\n",
    "sorted_documents = compute_similarity_tfidf(question, corpus)\n",
    "print(sorted_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
